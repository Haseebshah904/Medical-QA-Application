# -*- coding: utf-8 -*-
"""Groq LLM integrate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1loblspsApGDhf_qrAm0RhIZLH4F7uxgV
"""

from langchain.llms import BaseLLM
from typing import List, Dict, Any
import requests
from pydantic import Field
from langchain.schema import (
    Generation,
    LLMResult,
)

class GroqLLM(BaseLLM):
    """Groq large language model."""

    api_key: str = Field(..., description="YOUR_GROQ_API_KEY")
    api_url: str = Field(..., description="groq_api_url")
    temperature: float = Field(0.4, description="Temperature for response generation")
    max_tokens: int = Field(500, description="Maximum number of tokens in the response")
    model: str = Field("llama2-70b-chat", description="Model to use for completion")

    def _llm_type(self) -> str:
        """Return type of llm."""
        return "groq"

    def _call(self, prompt: str, stop: List[str] = None) -> str:
        """Call out to Groq's completion endpoint."""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "model": self.model,
        }

        if stop:
            payload["stop"] = stop

        response = requests.post(self.api_url, json=payload, headers=headers)

        if response.status_code == 200:
            return response.json().get("choices", [{}])[0].get("message", {}).get("content", "").strip()
        else:
            raise Exception(f"Groq API error: {response.text}")

    def _generate(self, prompts: List[str], stop: List[str] = None) -> LLMResult: # Change return type to LLMResult
        """Call out to Groq's completion endpoint for multiple prompts."""
        generations = []
        for prompt in prompts:
            text = self._call(prompt, stop=stop)
            generations.append([Generation(text=text)]) # Create Generation objects and append to generations

        return LLMResult(generations=generations) # Return an LLMResult object

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get the identifying parameters."""
        return {
            "api_key": self.api_key,
            "api_url": self.api_url,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "model": self.model,
        }

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# Define your Groq API details
groq_api_key = "Groq_API"
groq_api_url = "End point link"  # Removed -s from the URL

# Use the custom Groq LLM
# Update the model name to a valid and accessible model
llm = GroqLLM(api_key=groq_api_key, api_url=groq_api_url, temperature=0.4, max_tokens=500, model="llama3-8b-8192")

# Define the prompt
system_prompt = (
    "You are an assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer "
    "the question. If you don't know the answer, say that you "
    "don't know. Use three sentences maximum and keep the "
    "answer concise."
    "\n\n"
    "{context}"
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

# Create the chains
question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

# Example question
response = rag_chain.invoke({"input": "What is Acromegaly"})

output = response["answer"]

# Convert the response to LLResult
generation = Generation(text=output)
llm_result = LLMResult(generations=[[generation]])

print(llm_result)