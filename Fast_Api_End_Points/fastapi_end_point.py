# -*- coding: utf-8 -*-
"""Fastapi_End_point.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tdHuO5ggnaPTgfXreeSm5nOrSGO43_7k
"""

from pyngrok import ngrok
import uvicorn
import nest_asyncio
nest_asyncio.apply()

# Start the ngrok tunnel
ngrok.set_auth_token("   ")
public_url = ngrok.connect(8000).public_url
print(f"Public URL: {public_url}")


from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema import Generation, LLMResult
from langchain.llms import BaseLLM  # Import BaseLLM
from typing import List, Dict, Any, Optional
import requests

# GroqLLM class definition (copied from your notebook cell)
class GroqLLM(BaseLLM):  # Inherit from BaseLLM
    """Groq large language model."""

    api_key: str = Field(..., description="YOUR_GROQ_API_KEY")
    api_url: str = Field(..., description="groq_api_url")
    temperature: float = Field(0.4, description="Temperature for response generation")
    max_tokens: int = Field(500, description="Maximum number of tokens in the response")
    model: str = Field("llama3-8b-8192", description="Model to use for completion")

    def _llm_type(self) -> str:
        """Return type of llm."""
        return "groq"

    def _call(self, prompt: str, stop: List[str] = None) -> str:
        """Call out to Groq's completion endpoint."""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "model": self.model,
        }

        if stop:
            payload["stop"] = stop

        response = requests.post(self.api_url, json=payload, headers=headers)

        if response.status_code == 200:
            return response.json().get("choices", [{}])[0].get("message", {}).get("content", "").strip()
        else:
            raise Exception(f"Groq API error: {response.text}")

    def _generate(self, prompts: List[str], stop: List[str] = None) -> LLMResult:
        """Call out to Groq's completion endpoint for multiple prompts."""
        generations = []
        for prompt in prompts:
            text = self._call(prompt, stop=stop)
            generations.append([Generation(text=text)])
        return LLMResult(generations=generations)

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get the identifying parameters."""
        return {
            "api_key": self.api_key,
            "api_url": self.api_url,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "model": self.model,
        }
# End of GroqLLM class definition

# Initialize FastAPI app
app = FastAPI()

# Define your Groq API details
groq_api_key = "    "  # Replace with your actual API key
groq_api_url = "   "

# Initialize the custom LLM
llm = GroqLLM(api_key=groq_api_key, api_url=groq_api_url, temperature=0.4, max_tokens=500)

# Define the prompt template
system_prompt = (
    "You are an assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer "
    "the question. If you don't know the answer, say that you "
    "don't know. Use three sentences maximum and keep the "
    "answer concise."
    "\n\n"
    "{context}"
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

# Mock retriever (replace with an actual retriever instance)
retriever =  docsearch.as_retriever(search_type="similarity", search_kwargs={"k":3})  # Update this with your retriever logic
question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

# Pydantic model for input validation
class QuestionRequest(BaseModel):
    question: str

# Define the FastAPI endpoint
@app.post("/ask")
def ask_question(request: QuestionRequest):
    try:
        # Use the input question to invoke the chain
        input_data = {"input": request.question}
        response = rag_chain.invoke(input_data)
        output = response.get("answer", "I'm not sure about the answer.")  # Handle missing key gracefully

        # Return the result
        return {"question": request.question, "answer": output}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
uvicorn.run(app, host="0.0.0.0", port=8000)